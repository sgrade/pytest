More reasons you can provide for you choices, the better
Naive approach - the first gets into your mind when you think about the problem
    usually there is a smarter/ better way to do it
Time and Space efficiency (complexity)
    it is always a trade-off between them
Worst, best, average case scenarios

List based collections
    Linked lists
        Application: when the numbers of items in the list is unknown
    Stack - LIFO
        a.append()
        a.pop()
    Queue - FIFO
        priority queue
        double-ended queue
    Double ended queue
        from collections import deque
        a.popleft()

Binary search
    https://ru.wikipedia.org/wiki/%D0%94%D0%B2%D0%BE%D0%B8%D1%87%D0%BD%D1%8B%D0%B9_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA
    number of iterations incremented AT every power of 2
    O(log(n))
        log(8) = 3
        In computing, the log base usually 2: so 2^3 = 8

Sorting
    https://en.wikipedia.org/wiki/Sorting_algorithm
    In-place: we don't use extra data structures to do the sorting
    Out-of-place: we do use extra data structures to do the sorting
    E.g. BUBBLE SORT - сравниваем первые два элемента, меняем местами, если надо; далее второй-третий, ... Начинаем
        заново. О = (n^2)
        https://en.wikipedia.org/wiki/Bubble_sort
    MERGE SORT - "divide and conquer type of algorithm"
        number of iterations incremented AFTER every power of 2
        O(n*log(n)) - n comparisons for log(n) steps
        Time efficiency of merge sort is better than in bubble sort,
        but space efficiency is less (we need new arrays to do the comparisons)
    QUICK SORT - in the majority of cases it will be the most efficient sorting algorithm
        Pivot
        Time complexity:
            Worst case complexity is like in bubble sort. It happens when arrays are near sorted.
            Average and best case complexity is as in merge sort.
        Space complexity:
            Space = O(l)

Everything we learnt before - LINEAR time search

Maps (like a Python dictionary)
    Set of unique keys
    Values for a key
    Hash - constant lookup
    Hash function example:
        Get last two digits in a large number (.e.g. 56), divide it by e.g. 10, get the remainder - it will be index
        Why last two digits? They will be most random (например, номера билетов начинаются обычно с одинаковых цифр)
    Collisions:
        When hash function gives same result (index) for several values, you can store these values in a list (bucket)
        Load Factor = Number of Entries / Number of Buckets
            Low load factor - less collisions, but waste of memory
            Any table with a load value greater than 1 is guaranteed to have collisions.
        There is also a trade-off between complexity of hash function and number of collisions
    Hash maps:
        Map has key, value. Remember that keys are unique (they belong to a set)
        Apply hash function on key
        Store hash value (key, value) in hash value
    String keys:
        31
        s[0]*31*(n-1) + s[1]*31*(n-2) + s[2]*31*(n-3) + ... + s*(n-1)

Trees
    Root, branches, leaves. A collection of trees is forest.
    Can be considered as a linked list, but instead of one next element, there might be several.
    There should be no cycles in a tree.
    Leaves - nodes, that don't have any children
    Parent called internal node.
    Height - inverse to depth.
    Path
    Traversal on a tree
        DFS: depth first search (поиск в глубину). Note: order is left to right
            is an algorithm for traversing or searching tree or graph data structures.
                One starts at the root (selecting some arbitrary node as the root in the case of a graph) and
                explores as far as possible along each branch before backtracking.
                https://en.wikipedia.org/wiki/Depth-first_search
            Types:
                Pre-order: check off a node as you see it before you traverse any further in the tree
                In-order: check off a node if we've seen it left child
                Post-order: check a node if we've seen all of its descendants
        BFS: breadth-first search (поиск в ширину)
            is an algorithm for traversing or searching tree or graph data structures.
                It starts at the tree root (or some arbitrary node of a graph,
                sometimes referred to as a 'search key'[1]) and explores the neighbor nodes first,
                before moving to the next level neighbours.
                https://en.wikipedia.org/wiki/Breadth-first_search
            By convention, we start on a the left most side of the level and move right
    Search and delete
        time is linear
    BINARY TREE: every parent node has at most TWO children
        BST (binary search tree): sorted
            Search: check element. If it is less than target, search right, else - left.
                Best case: O(log(n))
                Worst case: O(n)
    HEAP
        Root is the maximum or the minimum depending on the type of heap: MAX HEAP and MIN HEAP
            MIN HEAP: parent has a lower value than its children (root is a minimum element)
            MAX HEAP: parent has a greater value than its children
        Heaps don't need to be binary, but in examples the teacher uses binary ones
        Insert: from left to right until the level is full
        Peek - maximum value
        Search: we don't check children, if the value we are looking for is greater then (max heap) than the node
        Time: Worst O(n), average about O(n/2)
        Heapify: we insert new element to the free slot in a heap (see insert above);
            then we check if it's parent is less or greater - if less, we swap
        Extract: similar as in heapify. When we delete parent, we move up its rightmost child, then compare it with
            its new children and do swaps when necessary.
            Worst case: O(log(n)) - height of the tree
    Heaps in Python
        https://docs.python.org/3/library/heapq.html
        Heaps are binary trees for which every parent node has a value less than or equal to any of its children.
        A priority queue is common use for a heap.
        The API ... differs from textbook heap algorithms in two aspects:
            (a) We use zero-based indexing.
            (b) Our pop method returns the smallest item, not the largest (called a “min heap” in textbooks;
            a “max heap” is more common in texts because of its suitability for in-place sorting).
            These two make it possible to view the heap as a regular Python list without surprises:
            heap[0] is the smallest item, and heap.sort() maintains the heap invariant
        Priority queue is an abstract data type which is like a regular queue or stack data structure,
            but where additionally each element has a "priority" associated with it.
            In a priority queue, an element with high priority is served before an element with low priority.
            If two elements have the same priority, they are served according to their order in the queue.
            https://en.wikipedia.org/wiki/Priority_queue
        Applications:
            Tournament brackets (олимпийская система)
            Schedulers
            Big disk sorts

    Balanced trees
        Extreme case of unbalanced tree is linked list
        Self-balancing tree - tries to minimize the number of levels that it uses
        Red-black tree:
                nodes are assigned an additional property (color)
                every node that doesn't have a leaf, must have a null child
                all leaf nodes are black
                if a node is red, both of its children must be black
                the root node must be black
                every path from a node to its descendant null nodes must contain the same number of black nodes
            Insert: red node only (if root, change it back to black); if the parent is red, change it to black;
                rearrange (left, right) to keep red-black properties satisfied

Graph
    Vertex (node) + edge (ребро)
        Edges often store information about strength of the connection - weighted edges
    Graph is a tree with cycles. However, it is difficult to go through graph with cycles - loops
    Directed graph
        DAG - directed acyclic graph (no cycles)
    Connected graph - no disconnected vertices. Disconnected - the opposite.
    Connectivity - minimum number of elements that need to be removed for a graph to become disconnected
    A directed graph is weakly connected when only replacing all of the directed edges with undirected
        edges can cause it to be connected.
    Strongly connected directed graphs must have a path from every node and every other node.
    Graph representation:
        Edge list: [[0,1], [1,2], [1,3], [2,3]]. Numbers correspond to vertices IDs. 2D list (list of lists).
        Adjacency list: [[1], [0,2,3], [1,3], [1,2]].
            Index - node, corresponding list stores all of it's adjacencies. 2D
        Adjacency matrix: [[0,1,0,0], [1,0,1,1], [0,1,0,1], [0,1,1,0]].
            Outer index - node, inner index - node (1 if connection.)
    DFS:
        Search a node (any as there is no root), then pick an edge, next node, next edge, next node until there is no
            edges. Then back, another edge, ...
        Common implementations: using stack and recursion
        Efficiency: O(|E| + |V|) - number of edges plus number of vertices

    BFS:
        Search a node, then all edges; when there is no edges left, new node
        Common implementation: using queue
        BFS is like we are creating a tree from a graph
        Efficiency: O(|E| + |V|) - number of edges plus number of vertices
            (we are visiting every edge and every vertex during our traversal)
    Paths:
        Eulerian - traverses each node at least once
            Not every graph is capable of having it.
            Graph has a eularian path if all vertices have an even degree or an even number of edges connected to them.
                Two nodes can have an odd degree if they are a start and end of the path.
            Find such path: ABC + BCD = ABCDE. Efficiency O(|E|)
        Hamiltonian - must go through every vertex once


Case studies
    Shortest path problem
        When the (graph) edges are not weighted, it's BFS
    Dijkstra algorithm
        Edges have 'distance'
        Common implementation: min priority queue
            We start from some node (distance zero); all other nodes in our list of nodes have value of infinity
            Remove the node with the least distance from the queue - node zero
            Then follow each edge and update value in the list
            Which node should we follow? The one with the smallest value - we apply extract min on the queue
            Because we always want to have the node with least cost - it's a greedy algorithm
        Efficiency O(|V|^2)
            Or, if a priority queue is implemented very efficienty, O(|E|+|V|*log(|V|))
    Knapsack problem
        Есть больше вещей, чем влезет в рюкзак. Надо положить maximum value for max weight
        Each item has a weight and a value
        How can I optimize the value given the weight constraint?
        0-1 knapsack problem: only 1 item of each type, we can take or leave the whole item
        Implementations:
            "Brute force" solution: O(2^N), where n - number of objects
            Faster algorithm: max value for smallest weight possible
                Indexes in the array represent weight
                O(n*W), where W - weight limit, n - number of elements
        Polynomial type of algorithms are much faster than exponential ones
    Dynamic programming (also known as dynamic optimization)
        is a method for solving a complex problem by breaking it down into a collection of simpler subproblems,
        solving each of those subproblems just once, and storing their solutions.
        The next time the same subproblem occurs, instead of recomputing its solution,
        one simply looks up the previously computed solution,
        thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space.
        (Each of the subproblem solutions is indexed in some way,
        typically based on the values of its input parameters, so as to facilitate its lookup.)
        The technique of storing solutions to subproblems instead of recomputing them is called "memoization".
    TSP - travelling salesman problem
        fastest way for a salesman to travel between every city and return home?
        NP-hard - don'have an algorithm to solve in polynomial time (where n has a constant exponent)
        Pseudo-polynomial
        Types of solution:
            Exact answer, but no polynomial time
            Approximation algorithms - find near optimal solution

